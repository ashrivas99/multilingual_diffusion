{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashrivas99/multilingual_diffusion/blob/main/glyffuser%20conditional_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accelerate==0.29.3\n",
        "# datasets==2.19.0\n",
        "# diffusers==0.28.0\n",
        "# einops==0.8.0\n",
        "# Pillow==10.3.0\n",
        "# torch==2.2.0\n",
        "# torchvision==0.17.0\n",
        "# tqdm==4.66.2\n",
        "# transformers==4.40.1\n",
        "\n",
        "!pip install datasets pillow"
      ],
      "metadata": {
        "id": "KdKI_gTt7mvr",
        "outputId": "80943fdf-b25a-49a1-f208-688cc1798d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation\n"
      ],
      "metadata": {
        "id": "XYnzwsN07BlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, random, os, sys, json, urllib.request, time, hashlib\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# ─────────────── KNOBS ────────────────\n",
        "CSV_FILE        = \"multilingual_data.csv\"\n",
        "OUT_DIR         = Path(\"data\")\n",
        "IMAGE_SIZE      = 128\n",
        "IMAGES_PER_CHAR = 5\n",
        "\n",
        "MINI, MINI_CODES, MINI_COPIES = False, [\"0041\",\"0641\",\"0915\"], 10\n",
        "# ──────────────────────────────────────\n",
        "\n",
        "# 1⃣  ——————— font download (no fallback to system fonts) ———————\n",
        "FONTS = {\n",
        "    \"latin\"     : (\"NotoSans-Regular.ttf\",\n",
        "                   \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf\"),\n",
        "    \"arabic\"    : (\"NotoNaskhArabic-Regular.ttf\",\n",
        "                   \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoNaskhArabic/NotoNaskhArabic-Regular.ttf\"),\n",
        "    \"devanagari\": (\"NotoSansDevanagari-Regular.ttf\",\n",
        "                   \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\"),\n",
        "}\n",
        "def dl_font(fname, url, retries=2):\n",
        "    \"\"\"download to `fname`, verify magic bytes 00 01 00 00 (TTF)\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        if Path(fname).exists():\n",
        "            with open(fname, \"rb\") as f:\n",
        "                if f.read(4) == b\"\\x00\\x01\\x00\\x00\":   # valid TrueType\n",
        "                    return fname\n",
        "                else:\n",
        "                    Path(fname).unlink()               # delete bogus file\n",
        "        print(f\"Downloading {fname}…\")\n",
        "        req = urllib.request.Request(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "        with urllib.request.urlopen(req) as r, open(fname, \"wb\") as f:\n",
        "            f.write(r.read())\n",
        "        time.sleep(0.2)  # give Colab FS a breath\n",
        "    raise RuntimeError(f\"❌ could not fetch a valid TTF for {fname}\")\n",
        "\n",
        "for ttf, link in FONTS.values():\n",
        "    dl_font(ttf, link)\n",
        "\n",
        "# 2⃣  ——————— helpers ———————\n",
        "def pick_font(ch):\n",
        "    cp = ord(ch)\n",
        "    if   0x0600 <= cp <= 0x06FF: ttf = FONTS[\"arabic\"][0]\n",
        "    elif 0x0900 <= cp <= 0x097F or cp == 0x0950: ttf = FONTS[\"devanagari\"][0]\n",
        "    else: ttf = FONTS[\"latin\"][0]\n",
        "    return ImageFont.truetype(ttf, IMAGE_SIZE-28)\n",
        "\n",
        "def render(ch, font):\n",
        "    img  = Image.new(\"L\", (IMAGE_SIZE, IMAGE_SIZE), 255)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    w,h  = draw.textbbox((0,0), ch, font=font)[2:]\n",
        "    draw.text(((IMAGE_SIZE-w)//2, (IMAGE_SIZE-h)//2-8), ch, font=font, fill=0)\n",
        "    return img\n",
        "\n",
        "def check_row(row):\n",
        "    return int(row.Unicode,16)==ord(row.Character)\n",
        "\n",
        "# 3⃣  ——————— build dataset ———————\n",
        "def build(csv_file=CSV_FILE, out_dir=OUT_DIR,\n",
        "          images_per_char=IMAGES_PER_CHAR,\n",
        "          mini=MINI, mini_codes=MINI_CODES, mini_copies=MINI_COPIES):\n",
        "\n",
        "    df = pd.read_csv(csv_file, dtype=str)\n",
        "    assert df.apply(check_row, axis=1).all(), \"Unicode ↔ glyph mismatch\"\n",
        "\n",
        "    if mini:\n",
        "        df = df[df.Unicode.isin(mini_codes)].reset_index(drop=True)\n",
        "        df = pd.concat([df]*mini_copies).reset_index(drop=True)\n",
        "        df[\"dup\"] = df.groupby(\"Unicode\").cumcount()\n",
        "        df[\"file_name\"] = df.Unicode+\"_\"+df.dup.astype(str).str.zfill(3)+\".png\"\n",
        "    else:\n",
        "        df[\"file_name\"] = df.Unicode+\".png\"\n",
        "        df = pd.concat([df]*images_per_char).reset_index(drop=True)\n",
        "\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for row in df.drop_duplicates(\"file_name\").itertuples(index=False):\n",
        "        render(row.Character, pick_font(row.Character)).save(out_dir/row.file_name)\n",
        "\n",
        "    df[[\"file_name\",\"caption\"]].to_json(out_dir/\"metadata.jsonl\",\n",
        "                                        orient=\"records\",lines=True,\n",
        "                                        force_ascii=False)\n",
        "    df.to_csv(out_dir/\"char_dataset.csv\", index=False)\n",
        "    print(f\"✅  {len(df)} rows | {df.file_name.nunique()} PNGs → {out_dir}\")\n",
        "\n",
        "build()  # run it"
      ],
      "metadata": {
        "id": "tPedLTB89-5J",
        "outputId": "45f923f4-9a7d-4a74-c493-ec697eeacb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  150 rows | 30 PNGs → data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "zdag5feF-vrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from typing import List\n",
        "from transformers import T5Tokenizer, T5EncoderModel, T5Config\n",
        "from einops import rearrange\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "# config\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "DEFAULT_T5_NAME = 'google/t5-v1_1-base'\n",
        "\n",
        "T5_CONFIGS = {}\n",
        "\n",
        "# singleton globals\n",
        "\n",
        "def get_tokenizer(name):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n",
        "    return tokenizer\n",
        "\n",
        "def get_model(name):\n",
        "    model = T5EncoderModel.from_pretrained(name)\n",
        "    return model\n",
        "\n",
        "def get_model_and_tokenizer(name):\n",
        "    global T5_CONFIGS\n",
        "\n",
        "    if name not in T5_CONFIGS:\n",
        "        T5_CONFIGS[name] = dict()\n",
        "    if \"model\" not in T5_CONFIGS[name]:\n",
        "        T5_CONFIGS[name][\"model\"] = get_model(name)\n",
        "    if \"tokenizer\" not in T5_CONFIGS[name]:\n",
        "        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n",
        "\n",
        "    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n",
        "\n",
        "def get_encoded_dim(name):\n",
        "    if name not in T5_CONFIGS:\n",
        "        # avoids loading the model if we only want to get the dim\n",
        "        config = T5Config.from_pretrained(name)\n",
        "        T5_CONFIGS[name] = dict(config=config)\n",
        "    elif \"config\" in T5_CONFIGS[name]:\n",
        "        config = T5_CONFIGS[name][\"config\"]\n",
        "    elif \"model\" in T5_CONFIGS[name]:\n",
        "        config = T5_CONFIGS[name][\"model\"].config\n",
        "    else:\n",
        "        assert False\n",
        "    return config.d_model\n",
        "\n",
        "# encoding text\n",
        "\n",
        "def t5_tokenize(\n",
        "    texts: List[str],\n",
        "    name = DEFAULT_T5_NAME\n",
        "):\n",
        "    t5, tokenizer = get_model_and_tokenizer(name)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        t5 = t5.cuda()\n",
        "\n",
        "    device = next(t5.parameters()).device\n",
        "\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        return_tensors = \"pt\",\n",
        "        padding = 'longest',\n",
        "        max_length = MAX_LENGTH,\n",
        "        truncation = True\n",
        "    )\n",
        "\n",
        "    input_ids = encoded.input_ids.to(device)\n",
        "    attn_mask = encoded.attention_mask.to(device)\n",
        "    return input_ids, attn_mask\n",
        "\n",
        "def t5_encode_tokenized_text(\n",
        "    token_ids,\n",
        "    attn_mask = None,\n",
        "    pad_id = None,\n",
        "    name = DEFAULT_T5_NAME\n",
        "):\n",
        "    assert exists(attn_mask) or exists(pad_id)\n",
        "    t5, _ = get_model_and_tokenizer(name)\n",
        "\n",
        "    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n",
        "\n",
        "    t5.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = t5(input_ids = token_ids, attention_mask = attn_mask)\n",
        "        encoded_text = output.last_hidden_state.detach()\n",
        "\n",
        "    attn_mask = attn_mask.bool()\n",
        "\n",
        "    encoded_text = encoded_text.masked_fill(~rearrange(attn_mask, '... -> ... 1'), 0.) # just force all embeddings that is padding to be equal to 0.\n",
        "    return encoded_text\n",
        "\n",
        "def t5_encode_text(\n",
        "    texts: List[str],\n",
        "    name = DEFAULT_T5_NAME,\n",
        "    return_attn_mask = False\n",
        "):\n",
        "    token_ids, attn_mask = t5_tokenize(texts, name = name)\n",
        "    encoded_text = t5_encode_tokenized_text(token_ids, attn_mask = attn_mask, name = name)\n",
        "\n",
        "    if return_attn_mask:\n",
        "        attn_mask = attn_mask.bool()\n",
        "        return encoded_text, attn_mask\n",
        "\n",
        "    return encoded_text\n"
      ],
      "metadata": {
        "id": "ziVvwgFU-vVg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "rvg6sLb492Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from diffusers.utils.torch_utils import randn_tensor\n",
        "\n",
        "\n",
        "# Collator adjusted for local dataset\n",
        "class Collator:\n",
        "    def __init__(self, image_size, text_label, image_label, name, channels):\n",
        "        self.text_label = text_label\n",
        "        self.image_label = image_label\n",
        "        self.name = name\n",
        "        self.channels = channels\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((image_size, image_size)),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        texts = []\n",
        "        masks = []\n",
        "        images = []\n",
        "        for item in batch:\n",
        "            try:\n",
        "                # Load image from local file\n",
        "                image_path = 'data/'+item[self.image_label]  # Assuming this is a path to the image file\n",
        "                with Image.open(image_path) as img:\n",
        "                    image = self.transform(img.convert(self.channels))\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process image {image_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Encode the text\n",
        "            text, mask = t5_encode_text(\n",
        "                [item[self.text_label]],\n",
        "                name=self.name,\n",
        "                return_attn_mask=True\n",
        "                )\n",
        "            texts.append(torch.squeeze(text))\n",
        "            masks.append(torch.squeeze(mask))\n",
        "            images.append(image)\n",
        "\n",
        "        if len(texts) == 0:\n",
        "            return None\n",
        "\n",
        "        # Are these strictly necessary?\n",
        "        texts = pad_sequence(texts, True)\n",
        "        masks = pad_sequence(masks, True)\n",
        "\n",
        "        newbatch = []\n",
        "        for i in range(len(texts)):\n",
        "            newbatch.append((images[i], texts[i], masks[i]))\n",
        "\n",
        "        return torch.utils.data.dataloader.default_collate(newbatch)\n",
        "\n",
        "\n",
        "class GlyffuserPipeline(DiffusionPipeline):\n",
        "    r'''\n",
        "    Pipeline for text-to-image generation from the glyffuser model\n",
        "\n",
        "    Parameters:\n",
        "        unet (['UNet2DConditionModel'])\n",
        "        scheduler (['SchedulerMixin'])\n",
        "        text_encoder (['TextEncoder']) - T5 small\n",
        "    '''\n",
        "    def __init__(self, unet, scheduler):\n",
        "        super().__init__()\n",
        "        self.register_modules(\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        text_encoder: str = \"google-t5/t5-small\",\n",
        "        batch_size: int = 1,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        num_inference_steps: int = 1000,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
        "        '''\n",
        "        Docstring\n",
        "        '''\n",
        "        # Get text embeddings\n",
        "        # Encode the text\n",
        "        # text_embeddings = []\n",
        "        # for text in texts:\n",
        "        #     embedding = t5.t5_encode_text(text, name=text_encoder)\n",
        "        #     text_embeddings.append(torch.squeeze(embedding))\n",
        "        # text_embeddings = pad_sequence(text_embeddings, True)\n",
        "\n",
        "        batch_size = len(texts)\n",
        "\n",
        "        text_embeddings, masks = t5_encode_text(texts, name=text_encoder, return_attn_mask=True)\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        if isinstance(self.unet.config.sample_size, int):\n",
        "            image_shape = (\n",
        "                batch_size,\n",
        "                self.unet.config.in_channels,\n",
        "                self.unet.config.sample_size,\n",
        "                self.unet.config.sample_size,\n",
        "            )\n",
        "        else:\n",
        "            image_shape = (batch_size, self.unet.config.in_channels, *self.unet.config.sample_size)\n",
        "\n",
        "\n",
        "        # if self.device.type == \"mps\": # MPS is apple silicon\n",
        "        #     # randn does not work reproducibly on mps\n",
        "        #     image = randn_tensor(image_shape, generator=generator)\n",
        "        #     image = image.to(self.device)\n",
        "        # else:\n",
        "        image = randn_tensor(image_shape, generator=generator, device=self.device)\n",
        "\n",
        "        # set step values\n",
        "        self.scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "        for t in self.progress_bar(self.scheduler.timesteps):\n",
        "            # 1. predict noise model_output\n",
        "            model_output = self.unet(\n",
        "                image,\n",
        "                t,\n",
        "                encoder_hidden_states=text_embeddings, # Add text encoding input\n",
        "                encoder_attention_mask=masks, # Add attention mask\n",
        "                return_dict=False\n",
        "                )[0] # <-- sample is an attribute of the BaseOutClass of type torch.FloatTensor\n",
        "\n",
        "            # 2. compute previous image: x_t -> x_t-1\n",
        "            image = self.scheduler.step(model_output, t, image, generator=generator, return_dict=False)[0]\n",
        "\n",
        "        # image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.clamp(0, 1) # No need to rescale for HF yuewu/glyffuser\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image,)\n",
        "\n",
        "        return ImagePipelineOutput(images=image)\n",
        "\n",
        "def make_grid(images, rows, cols):\n",
        "    w, h = images[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    for i, image in enumerate(images):\n",
        "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def evaluate(config, epoch, texts, pipeline):\n",
        "    images = pipeline(\n",
        "        texts,\n",
        "        batch_size = config.eval_batch_size,\n",
        "        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Generator must be on CPU for sampling during training\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n",
        "\n",
        "def make_labeled_grid(images, prompt, steps, font_path=None, font_size=20, margin=10):\n",
        "    assert len(images) == len(steps), \"The number of images must match the number of steps\"\n",
        "\n",
        "    w, h = images[0].size\n",
        "    font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()\n",
        "\n",
        "    # Calculate the height of the grid including the margin for text\n",
        "    total_height = h + margin + font_size\n",
        "    total_width = w * len(images)\n",
        "    grid_height = total_height + margin + font_size  # Add extra margin for the prompt\n",
        "    grid = Image.new('RGB', size=(total_width, grid_height), color=(255, 255, 255))\n",
        "\n",
        "    # Draw the text prompt at the top\n",
        "    draw = ImageDraw.Draw(grid)\n",
        "    prompt_text = f\"Prompt: \\\"{prompt}\\\"\"\n",
        "    prompt_width, prompt_height = draw.textbbox((0, 0), prompt_text, font=font)[2:4]\n",
        "    prompt_x = (total_width - prompt_width) / 2\n",
        "    prompt_y = margin / 2\n",
        "    draw.text((prompt_x, prompt_y), prompt_text, fill=\"black\", font=font)\n",
        "\n",
        "    for i, (image, step) in enumerate(zip(images, steps)):\n",
        "        # Calculate position to paste the image\n",
        "        x = i * w\n",
        "        y = margin + font_size\n",
        "\n",
        "        # Paste the image\n",
        "        grid.paste(image, box=(x, y))\n",
        "\n",
        "        # Draw the step text\n",
        "        step_text = f\"Steps: {step}\"\n",
        "        text_width, text_height = draw.textbbox((0, 0), step_text, font=font)[2:4]\n",
        "        text_x = x + (w - text_width) / 2\n",
        "        text_y = y + h + margin / 2 - 8\n",
        "        draw.text((text_x, text_y), step_text, fill=\"black\", font=font)\n",
        "\n",
        "    return grid"
      ],
      "metadata": {
        "id": "HH0rWWJv9383"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011UQtUq6__T"
      },
      "source": [
        "# Conditional Glyffuser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTLWx9cJ6__V"
      },
      "source": [
        "### Define training parameters\n",
        "We add text_encoder and encoder_dim parameters to the config. The batch size is also smaller due to the larger size of the conditional model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YtyPhlN06__V"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = 128  # the generated image resolution\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16  # how many images to sample during evaluation\n",
        "    num_epochs = 100\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 10\n",
        "    save_model_epochs = 30\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = \"glyffuser\"  # the model name\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    text_encoder = \"google-t5/t5-small\"\n",
        "    encoder_dim = 1024\n",
        "    seed = 0\n",
        "\n",
        "config = TrainingConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XICc2o1S6__W"
      },
      "source": [
        "Additional plumbing has been moved to the `glyffuser_utils` module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO143ea_6__W",
        "outputId": "c4da1d3d-cbe9-4fda-9eef-8cdeb8ac62e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\y_w_u\\anaconda3\\envs\\torchcuda12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
        "from accelerate import Accelerator\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "\n",
        "        if config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything for accelerator\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch[0]\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "            with accelerator.accumulate(model):\n",
        "                noise_pred = model(\n",
        "                    noisy_images,\n",
        "                    timesteps,\n",
        "                    encoder_hidden_states=batch[1],\n",
        "                    return_dict=False\n",
        "                    )[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = GlyffuserPipeline(\n",
        "                unet=accelerator.unwrap_model(model),\n",
        "                scheduler=inference_scheduler)\n",
        "            texts=[ # Provides some text prompts for sampling\n",
        "                \"vicious, cruel; severely, extreme\",\n",
        "                \"panting of cow; grunting of ox\",\n",
        "                \"sing; folksong, ballad; rumor\",\n",
        "                \"a quiver\"\n",
        "            ]\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                pipeline = GlyffuserPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)\n",
        "                pipeline.save_pretrained(config.output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT_y4cZS6__X"
      },
      "source": [
        "We add a collator which helps the dataloader pass text embeddings to the model during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctUUywgZ6__X",
        "outputId": "17c5c1ff-93b1-46c7-c0fa-a22554a774f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118,923,137 total parameters.\n"
          ]
        }
      ],
      "source": [
        "# Define collator\n",
        "collator = Collator(\n",
        "    image_size=128,\n",
        "    text_label='caption',  # This is where you specify which field to use for text\n",
        "    image_label='file_name',\n",
        "    name=config.text_encoder,\n",
        "    channels='L'\n",
        ")\n",
        "\n",
        "# Define data source\n",
        "dataset = load_dataset(\"json\", data_files=\"data/metadata.jsonl\")\n",
        "train_dataloader = DataLoader(dataset['train'], batch_size=config.train_batch_size, collate_fn=collator, shuffle=True)\n",
        "\n",
        "# Define model\n",
        "model = UNet2DConditionModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=1,  # the number of input channels, 1 for RGB images\n",
        "    out_channels=1,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    # block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "    block_out_channels=(128, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "\n",
        "    addition_embed_type=\"text\", # Make it conditional\n",
        "    cross_attention_dim=config.encoder_dim,\n",
        "    encoder_hid_dim=config.encoder_dim,  # the hidden dimension of the encoder\n",
        "    encoder_hid_dim_type=\"text_proj\",  # the hidden dimension of the encoder\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"CrossAttnDownBlock2D\",\n",
        "        \"DownBlock2D\"\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",\n",
        "        \"CrossAttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Define optimizers and schedulers\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "inference_scheduler = DPMSolverMultistepScheduler()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")\n",
        "# Check model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{total_params:,} total parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbwq4wZn6__X"
      },
      "source": [
        "Pass everything to accelerate and run the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7Hpuxt-6__X"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "notebook_launcher(train_loop, args, num_processes=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}