{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional glyffuser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # assumes images are square\n",
    "    train_batch_size = 32\n",
    "    eval_batch_size = 32\n",
    "    num_epochs = 100\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 1\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"glyffuser-unconditional\"  # the model name\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "    dataset_name=\"data128\"\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def normalize_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "class LocalDataset(Dataset):\n",
    "    # A dataset that loads images from a folder\n",
    "    def __init__(self, folder, image_size, exts=['png']):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for ext in exts for p in Path(folder).glob(f'**/*.{ext}')]\n",
    "        self.transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(normalize_neg_one_to_one),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path).convert('L')  # Open the image in grayscale mode\n",
    "        return self.transform(img)\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    # Helper function for making a grid of images\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample from the model and save the images in a grid\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size, \n",
    "        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Generator must be on CPU for sampling during training\n",
    "        num_inference_steps=50\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the inverted images\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Training loop that can be passed to accelerate\n",
    "\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "    \n",
    "    # Prepare everything for accelerator\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    # Loop through each epoch\n",
    "    for epoch in range(config.num_epochs):\n",
    "\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)\n",
    "                evaluate(config, epoch, pipeline)\n",
    "            \n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)\n",
    "                save_dir = os.path.join(config.output_dir, f\"epoch{epoch}\")\n",
    "                pipeline.save_pretrained(save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data source\n",
    "dataset = LocalDataset(config.dataset_name, image_size=config.image_size)\n",
    "train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=1,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define optimizers and schedulers\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "inference_scheduler = DPMSolverMultistepScheduler()\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass everything to accelerate and run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/656 [00:00<?, ?it/s]c:\\Users\\y_w_u\\miniconda3\\envs\\torchcuda12\\Lib\\site-packages\\diffusers\\configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.34it/s]  2.49it/s, loss=0.0166, lr=0.0001, step=655] \n",
      "Epoch 0: 100%|██████████| 656/656 [04:38<00:00,  2.36it/s, loss=0.0166, lr=0.0001, step=655]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.54it/s]\n",
      "Epoch 1: 100%|██████████| 656/656 [03:37<00:00,  3.01it/s, loss=0.00739, lr=0.0001, step=1311]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.54it/s]  3.30it/s, loss=0.00661, lr=9.99e-5, step=1967]\n",
      "Epoch 2: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00661, lr=9.99e-5, step=1967]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.34it/s]\n",
      "Epoch 3: 100%|██████████| 656/656 [03:37<00:00,  3.02it/s, loss=0.00648, lr=9.97e-5, step=2623]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]  3.25it/s, loss=0.00526, lr=9.96e-5, step=3279]\n",
      "Epoch 4: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00526, lr=9.96e-5, step=3279]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "Epoch 5: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.0056, lr=9.93e-5, step=3935]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.34it/s]  3.28it/s, loss=0.00392, lr=9.9e-5, step=4591] \n",
      "Epoch 6: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00392, lr=9.9e-5, step=4591]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "Epoch 7: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00484, lr=9.87e-5, step=5247]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.33it/s]  3.28it/s, loss=0.005, lr=9.83e-5, step=5903]  \n",
      "Epoch 8: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.005, lr=9.83e-5, step=5903]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.34it/s]\n",
      "Epoch 9: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00334, lr=9.79e-5, step=6559]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s],  3.25it/s, loss=0.00433, lr=9.74e-5, step=7215]\n",
      "Epoch 10: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00433, lr=9.74e-5, step=7215]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "Epoch 11: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00422, lr=9.69e-5, step=7871]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s],  3.26it/s, loss=0.00368, lr=9.63e-5, step=8527]\n",
      "Epoch 12: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00368, lr=9.63e-5, step=8527]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "Epoch 13: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00741, lr=9.57e-5, step=9183]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.26it/s, loss=0.00866, lr=9.5e-5, step=9839] \n",
      "Epoch 14: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00866, lr=9.5e-5, step=9839]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "Epoch 15: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00424, lr=9.43e-5, step=10495]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.24it/s, loss=0.00347, lr=9.35e-5, step=11151]\n",
      "Epoch 16: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00347, lr=9.35e-5, step=11151]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 17: 100%|██████████| 656/656 [03:36<00:00,  3.04it/s, loss=0.00267, lr=9.27e-5, step=11807]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.27it/s, loss=0.00382, lr=9.19e-5, step=12463]\n",
      "Epoch 18: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00382, lr=9.19e-5, step=12463]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "Epoch 19: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00442, lr=9.1e-5, step=13119]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.27it/s, loss=0.00491, lr=9.01e-5, step=13775]\n",
      "Epoch 20: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00491, lr=9.01e-5, step=13775]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 21: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00317, lr=8.91e-5, step=14431]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.29it/s, loss=0.00264, lr=8.81e-5, step=15087]\n",
      "Epoch 22: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00264, lr=8.81e-5, step=15087]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 23: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00275, lr=8.71e-5, step=15743]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s],  2.68it/s, loss=0.0037, lr=8.6e-5, step=16399]  \n",
      "Epoch 24: 100%|██████████| 656/656 [04:15<00:00,  2.57it/s, loss=0.0037, lr=8.6e-5, step=16399]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Epoch 25: 100%|██████████| 656/656 [04:02<00:00,  2.70it/s, loss=0.00471, lr=8.49e-5, step=17055]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s],  3.28it/s, loss=0.00466, lr=8.37e-5, step=17711]\n",
      "Epoch 26: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00466, lr=8.37e-5, step=17711]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Epoch 27: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00355, lr=8.25e-5, step=18367]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s],  3.24it/s, loss=0.00371, lr=8.13e-5, step=19023]\n",
      "Epoch 28: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00371, lr=8.13e-5, step=19023]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 29: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00295, lr=8.01e-5, step=19679]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.51it/s],  3.27it/s, loss=0.00313, lr=7.88e-5, step=20335]\n",
      "Epoch 30: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00313, lr=7.88e-5, step=20335]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.52it/s]\n",
      "Epoch 31: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00273, lr=7.75e-5, step=20991]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.25it/s, loss=0.00279, lr=7.62e-5, step=21647]\n",
      "Epoch 32: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00279, lr=7.62e-5, step=21647]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 33: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00317, lr=7.48e-5, step=22303]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s],  3.23it/s, loss=0.00353, lr=7.34e-5, step=22959]\n",
      "Epoch 34: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00353, lr=7.34e-5, step=22959]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 35: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00373, lr=7.2e-5, step=23615]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s],  3.28it/s, loss=0.00393, lr=7.06e-5, step=24271]\n",
      "Epoch 36: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00393, lr=7.06e-5, step=24271]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 37: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00374, lr=6.91e-5, step=24927]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.27it/s, loss=0.00278, lr=6.76e-5, step=25583]\n",
      "Epoch 38: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00278, lr=6.76e-5, step=25583]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 39: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00322, lr=6.62e-5, step=26239]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s],  3.27it/s, loss=0.00252, lr=6.46e-5, step=26895]\n",
      "Epoch 40: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00252, lr=6.46e-5, step=26895]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 41: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00486, lr=6.31e-5, step=27551]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.24it/s, loss=0.00309, lr=6.16e-5, step=28207]\n",
      "Epoch 42: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00309, lr=6.16e-5, step=28207]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 43: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00322, lr=6.01e-5, step=28863]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.26it/s, loss=0.00302, lr=5.85e-5, step=29519]\n",
      "Epoch 44: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00302, lr=5.85e-5, step=29519]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 45: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00261, lr=5.69e-5, step=30175]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.28it/s, loss=0.00352, lr=5.54e-5, step=30831]\n",
      "Epoch 46: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00352, lr=5.54e-5, step=30831]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 47: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00245, lr=5.38e-5, step=31487]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.20it/s, loss=0.0038, lr=5.22e-5, step=32143] \n",
      "Epoch 48: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.0038, lr=5.22e-5, step=32143]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 49: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00276, lr=5.06e-5, step=32799]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s],  3.27it/s, loss=0.00327, lr=4.9e-5, step=33455] \n",
      "Epoch 50: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00327, lr=4.9e-5, step=33455]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 51: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00213, lr=4.75e-5, step=34111]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.27it/s, loss=0.00184, lr=4.59e-5, step=34767]\n",
      "Epoch 52: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00184, lr=4.59e-5, step=34767]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
      "Epoch 53: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.0024, lr=4.43e-5, step=35423]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s],  3.25it/s, loss=0.00295, lr=4.27e-5, step=36079]\n",
      "Epoch 54: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00295, lr=4.27e-5, step=36079]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "Epoch 55: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00298, lr=4.12e-5, step=36735]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.27it/s, loss=0.00265, lr=3.96e-5, step=37391]\n",
      "Epoch 56: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00265, lr=3.96e-5, step=37391]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 57: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00128, lr=3.81e-5, step=38047]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.31it/s],  3.28it/s, loss=0.00316, lr=3.66e-5, step=38703]\n",
      "Epoch 58: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00316, lr=3.66e-5, step=38703]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 59: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00386, lr=3.5e-5, step=39359]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.50it/s],  3.28it/s, loss=0.00221, lr=3.35e-5, step=4e+4] \n",
      "Epoch 60: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00221, lr=3.35e-5, step=4e+4]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.52it/s]\n",
      "Epoch 61: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00331, lr=3.2e-5, step=40671]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.27it/s, loss=0.00293, lr=3.06e-5, step=41327]\n",
      "Epoch 62: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00293, lr=3.06e-5, step=41327]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 63: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.0017, lr=2.91e-5, step=41983]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.26it/s, loss=0.00159, lr=2.77e-5, step=42639]\n",
      "Epoch 64: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00159, lr=2.77e-5, step=42639]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 65: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00348, lr=2.63e-5, step=43295]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.28it/s, loss=0.00333, lr=2.49e-5, step=43951]\n",
      "Epoch 66: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00333, lr=2.49e-5, step=43951]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 67: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00293, lr=2.36e-5, step=44607]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.24it/s, loss=0.00243, lr=2.22e-5, step=45263]\n",
      "Epoch 68: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00243, lr=2.22e-5, step=45263]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Epoch 69: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00216, lr=2.09e-5, step=45919]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.28it/s, loss=0.00216, lr=1.97e-5, step=46575]\n",
      "Epoch 70: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00216, lr=1.97e-5, step=46575]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 71: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00272, lr=1.84e-5, step=47231]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.28it/s, loss=0.00309, lr=1.72e-5, step=47887]\n",
      "Epoch 72: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00309, lr=1.72e-5, step=47887]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Epoch 73: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00264, lr=1.6e-5, step=48543]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.18it/s, loss=0.00248, lr=1.49e-5, step=49199]\n",
      "Epoch 74: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00248, lr=1.49e-5, step=49199]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 75: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00245, lr=1.38e-5, step=49855]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.25it/s, loss=0.00206, lr=1.27e-5, step=50511]\n",
      "Epoch 76: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00206, lr=1.27e-5, step=50511]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 77: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00342, lr=1.17e-5, step=51167]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.29it/s, loss=0.00185, lr=1.07e-5, step=51823]\n",
      "Epoch 78: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00185, lr=1.07e-5, step=51823]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 79: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00264, lr=9.72e-6, step=52479]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s],  3.23it/s, loss=0.00212, lr=8.8e-6, step=53135] \n",
      "Epoch 80: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00212, lr=8.8e-6, step=53135]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "Epoch 81: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.0021, lr=7.92e-6, step=53791]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.27it/s, loss=0.00215, lr=7.09e-6, step=54447]\n",
      "Epoch 82: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00215, lr=7.09e-6, step=54447]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 83: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00226, lr=6.3e-6, step=55103]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.26it/s, loss=0.00234, lr=5.55e-6, step=55759]\n",
      "Epoch 84: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00234, lr=5.55e-6, step=55759]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 85: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00208, lr=4.85e-6, step=56415]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.25it/s, loss=0.00242, lr=4.19e-6, step=57071]\n",
      "Epoch 86: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00242, lr=4.19e-6, step=57071]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 87: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00242, lr=3.58e-6, step=57727]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.27it/s, loss=0.00241, lr=3.02e-6, step=58383]\n",
      "Epoch 88: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00241, lr=3.02e-6, step=58383]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 89: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00238, lr=2.5e-6, step=59039]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.52it/s],  3.25it/s, loss=0.00326, lr=2.03e-6, step=59695]\n",
      "Epoch 90: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00326, lr=2.03e-6, step=59695]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.51it/s]\n",
      "Epoch 91: 100%|██████████| 656/656 [03:35<00:00,  3.04it/s, loss=0.00352, lr=1.61e-6, step=60351]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s],  3.28it/s, loss=0.00214, lr=1.23e-6, step=61007]\n",
      "Epoch 92: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00214, lr=1.23e-6, step=61007]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s]\n",
      "Epoch 93: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00203, lr=9.08e-7, step=61663]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.27it/s, loss=0.00183, lr=6.33e-7, step=62319]\n",
      "Epoch 94: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00183, lr=6.33e-7, step=62319]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Epoch 95: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.002, lr=4.06e-7, step=62975]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.29it/s],  3.24it/s, loss=0.00163, lr=2.3e-7, step=63631] \n",
      "Epoch 96: 100%|██████████| 656/656 [03:34<00:00,  3.06it/s, loss=0.00163, lr=2.3e-7, step=63631]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "Epoch 97: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00148, lr=1.03e-7, step=64287]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.30it/s],  3.28it/s, loss=0.00214, lr=2.67e-8, step=64943]\n",
      "Epoch 98: 100%|██████████| 656/656 [03:34<00:00,  3.05it/s, loss=0.00214, lr=2.67e-8, step=64943]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Epoch 99: 100%|██████████| 656/656 [03:35<00:00,  3.05it/s, loss=0.00176, lr=2.57e-11, step=65599]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a model and run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"glyffuser-unconditional\" # Path to the model directory\n",
    "pipeline = DDPMPipeline.from_pretrained(model_path).to(\"cuda\")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler()\n",
    "\n",
    "# Sample from the model and save the images in a grid\n",
    "images = pipeline(\n",
    "    batch_size=16, \n",
    "    generator=torch.Generator(device='cuda').manual_seed(config.seed), # Generator can be on GPU here\n",
    "    num_inference_steps=50\n",
    ").images\n",
    "\n",
    "# Make a grid out of the inverted images\n",
    "image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "# Save the images\n",
    "test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "image_grid.save(f\"{test_dir}/samples.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
